{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "import lasio\n",
    "import glob \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n",
    "%load_ext autotime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of las files in the folder\n",
    "\n",
    "path = 'Train Data set'\n",
    "files = glob.glob(path + \"/*.las\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create well-wise dataframes and populate with logs from the las files\n",
    "\n",
    "i = 1\n",
    "\n",
    "for file in files:\n",
    "    las = lasio.read(file)\n",
    "    globals()[str(\"Well_\"+str(i))] = las.df().reset_index().fillna(0)\n",
    "    globals()[str(\"Well_\"+str(i))]['Well Name'] = \"Well_\"+str(i)\n",
    "    globals()[str(\"Well_\"+str(i))]['Latitude'] = las.well.SLAT.value\n",
    "    globals()[str(\"Well_\"+str(i))]['Longitude'] = las.well.SLON.value\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create exploratory dataframe with surface locations and available logs to help us identify the inputs\n",
    "\n",
    "wells = []\n",
    "lat = []\n",
    "lon = []\n",
    "keys = []\n",
    "logcount = []\n",
    "units = []\n",
    "uniqueunits = []\n",
    "descrs = []\n",
    "depth = []\n",
    "i = 1\n",
    "\n",
    "for file in files:\n",
    "    well = \"Well_\"+str(i)\n",
    "    las = lasio.read(file)\n",
    "    latitude = las.well.SLAT.value\n",
    "    longitude = las.well.SLON.value\n",
    "    key = las.keys()\n",
    "    md = las.well.STOP.value\n",
    "    keycount = len(key)\n",
    "    unit = []\n",
    "    descr = []\n",
    "    for j in range(keycount):\n",
    "        unit.append(las.curves.items()[j][1].unit)\n",
    "        descr.append(las.curves.items()[j][1].descr)\n",
    "        j+=1\n",
    "    i+=1\n",
    "    uniqueunit = list(set(unit))\n",
    "    lat.append(latitude)\n",
    "    lon.append(longitude)\n",
    "    depth.append(md)\n",
    "    keys.append(key)\n",
    "    logcount.append(keycount)\n",
    "    wells.append(well)\n",
    "    units.append(unit)\n",
    "    uniqueunits.append(uniqueunit)\n",
    "    descrs.append(descr)\n",
    "    \n",
    "welldf = DataFrame({'Wellname':wells,'Latitude':lat,'Longitude':lon,'Depth':depth, 'Log Count':logcount,'Log List':keys,\n",
    "                    'Unit List':units, 'Unique Units':uniqueunits, 'Description': descrs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the lat,long and log count to get an idea on how the data is distributed\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.scatterplot(x ='Latitude', y = 'Longitude', data = welldf, size= 'Log Count', sizes = (100,600))\n",
    "\n",
    "# We see that there are two clusters of wells based on locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a histogram to see the number of logs available in each \n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.histplot(welldf['Log Count'], bins = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the log list column in our exploratory DF\n",
    "# We can try to figure out if the naming schemes of all the well logs is similar.\n",
    "\n",
    "welldf_explode = welldf.explode('Log List')\n",
    "welldf_explode['Units'] = list(welldf.explode('Unit List')['Unit List'])\n",
    "welldf_explode['Description'] = list(welldf.explode('Description')['Description'])\n",
    "welldf_explode.rename(columns = {'Log List':'Logs'}, inplace = True)\n",
    "welldf_explode.drop(['Unit List'],axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the logs available based on mnemonics \n",
    "plt.figure(figsize=(40,7))\n",
    "welldf_explode['Logs'].value_counts().plot.bar()\n",
    "plt.title('Log Mnemonic Count in 234 Wells')\n",
    "\n",
    "# We can see that the naming scheme is not really similar. \n",
    "# Only Depth and DTSM (target variable) are available for all the 234 wells.\n",
    "# Other logs are either named differntly or unavailable. We might have to rename some of the wells and condition the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can plot the number of logs based on units listed in the las files\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "welldf_explode['Units'].value_counts().plot.bar()\n",
    "plt.title('Unit Count in 234 Wells')\n",
    "\n",
    "# We can see that there are multiple logs with same units making the highest log count to 700 when the number of wells are 234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets plot the only the unique log units available in a well\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "welldf.explode('Unique Units')['Unique Units'].value_counts().plot.bar()\n",
    "plt.title('Entire Well')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection and  Compound Feature Creation\n",
    "\n",
    "Based on the log availability, we can select the following freatures initially\n",
    "\n",
    "1. LAT, LONG: Relative location to include spatial variation\n",
    "2. DEPT: Available for all the wells, might have to eliminate some laterals from the training set\n",
    "3. DTSM: Available for all the wells, one well has DTSM with different units, check magnitude and eliminate if necessary\n",
    "4. DTCO: Available for 223 Wells, we can impute the rest\n",
    "5. RHOB: Available for 169 wells, some of them are density correction logs which needs to be removed and rest needs to be imputed\n",
    "6. PEF: Available for 160 wells, rest need to be imputed (They are named differently, so we can create a new feature with an average value if more than one is available in a well)\n",
    "7. GR: Available for 230 wells, an average GR feature needs to be created and imputed\n",
    "8. NPHI: Available for 199 wells, an average NPHI feature needs to be created and imputed\n",
    "9. RES: Available for 181 wells, but many different variations are available so depending on the magnitudes, we might have to eliminate the entire feature\n",
    "10. CALI: Caliper logs are available for 187 wells but might not be effective in predicting target variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the las files for all the wells into a single dataframe\n",
    "\n",
    "well_logs = []\n",
    "for i in range (1, len(wells)+1):\n",
    "    well_logs.append(globals()[\"Well_\"+str(i)])\n",
    "    \n",
    "welldf_entire = pd.concat(well_logs)\n",
    "\n",
    "# Replacing zero values with NaNs\n",
    "welldf_entire.replace(0,np.nan,inplace=True)\n",
    "\n",
    "# Replacing negative values with NaNs\n",
    "welldf_entire[welldf_entire.DTSM < 0] = np.nan\n",
    "\n",
    "# Dropping the columns where DTSM(target variable) is null\n",
    "welldf_entire.dropna(subset = ['DTSM'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the units, lets select the log mnemonics that we can create a compound feature\n",
    "\n",
    "GAPI = list(set(welldf_explode[welldf_explode['Units'] == \"GAPI\"]['Logs'].value_counts().index))\n",
    "\n",
    "# Remove sprectral gamma ray logs from the list of logs having units GAPI\n",
    "SGR = ['SGRDD','HSGRD','HSGRS','SGRS','HSGR','SGR','HSGRR','MSGRR','SGRD','SGRDD','SGRR','HCGR']\n",
    "\n",
    "for element in GAPI:\n",
    "    if element in SGR:\n",
    "        GAPI.remove(element)\n",
    "\n",
    "DEC = list(set(welldf_explode[welldf_explode['Units'] == \"DEC\"]['Logs'].value_counts().index))\n",
    "\n",
    "# One of the DTSM log has the units of DEC, removing it from the porosity logs\n",
    "DEC.remove('DTSM')\n",
    "\n",
    "IN = list(set(welldf_explode[welldf_explode['Units'] == \"IN\"]['Logs'].value_counts().index))\n",
    "\n",
    "OHMM = list(set(welldf_explode[welldf_explode['Units'] == \"OHMM\"]['Logs'].value_counts().index))\n",
    "\n",
    "GC3  = list(set(welldf_explode[welldf_explode['Units'] == \"G/C3\"]['Logs'].value_counts().index))\n",
    "\n",
    "# Remove density correction logs from the list of logs having units G/C3\n",
    "DCORR = ['HDRA','DRHO','DRH','ZCOR','DCOR','CORR','QRHO','QRHO_SLDT','QRHO']\n",
    "for element in GC3:\n",
    "    if element in DCORR:\n",
    "        GC3.remove(element)\n",
    "        \n",
    "BE = list(set(welldf_explode[welldf_explode['Units'] == \"B/E\"]['Logs'].value_counts().index))\n",
    "\n",
    "# MV = list(set(welldf_explode[welldf_explode['Units'] == \"MV\"]['Logs'].value_counts().index))\n",
    "# LB = list(set(welldf_explode[welldf_explode['Units'] == \"LB\"]['Logs'].value_counts().index))\n",
    "# MMHO = list(set(welldf_explode[welldf_explode['Units'] == \"MMHO\"]['Logs'].value_counts().index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the row average of the features to create average features if more than one kind of log is available\n",
    "\n",
    "welldf_entire['GR_average'] = welldf_entire[GAPI].mean(axis = 1)\n",
    "welldf_entire['PHI_average'] = welldf_entire[DEC].mean(axis = 1)\n",
    "welldf_entire['CALI_average'] = welldf_entire[IN].mean(axis = 1)\n",
    "welldf_entire['RES_average'] = welldf_entire[OHMM].mean(axis = 1)\n",
    "welldf_entire['RHOB_average'] = welldf_entire[GC3].mean(axis = 1)\n",
    "welldf_entire['PEF_average'] = welldf_entire[BE].mean(axis = 1)\n",
    "\n",
    "# welldf_entire['SP_average'] = welldf_entire[MV].mean(axis = 1)\n",
    "# welldf_entire['Tension_average'] = welldf_entire[LB].mean(axis = 1)\n",
    "# welldf_entire['Induction_average'] = welldf_entire[MMHO].mean(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a df with selected features\n",
    "\n",
    "features_entire =  welldf_entire[['DEPT','Latitude','Longitude','DTCO','RHOB_average','PEF_average',\n",
    "                                  'GR_average','PHI_average','CALI_average','RES_average', 'DTSM']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null check on final features selected, we will impute the points shown in yellow\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(features_entire.isnull(), cmap = 'plasma', yticklabels= False, cbar = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing negative values with nans \n",
    "for column in features_entire.columns:\n",
    "    features_entire[column].loc[features_entire[column] <= 0] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into train and test for self-validation for all three dataframes\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = features_entire.drop('DTSM', axis =1)\n",
    "y = features_entire.DTSM\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As this is a regression problem, the magnitude distribution can sometimes affect the model prediction. \n",
    "# So it will be ideal to scale the features\n",
    "# Then we can use the iterative imputer to fill the missing values of the scaled features\n",
    "# Then finally you can put this through ML model, Random Forest regressor in this case\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# define model pipeline\n",
    "\n",
    "scaler = RobustScaler()\n",
    "imputer = IterativeImputer(random_state= 42, missing_values = np.nan)\n",
    "model = XGBRegressor(random_state = 42, n_jobs= -1, verbosity= 2)\n",
    "pipeline = Pipeline(steps=[('s',scaler),('i', imputer), ('m', model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit the model to the training dataset\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions with the test dataset\n",
    "predictions = pipeline.predict(X_test)\n",
    "\n",
    "# evaluate model by calculating the RMSE\n",
    "rmse = np.sqrt(MSE(y_test, predictions))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-Parameter Tuning for increased Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hyper parameter tuning in order to improve the accuracy with a wider net\n",
    "\n",
    "# parameters = {'m__learning_rate': [.05, 0.1, .15],\n",
    "#               'm__n_estimators': [500, 1000, 1500],\n",
    "#               'm__max_depth': [5, 10, 20],\n",
    "#               'm__min_child_weight': [5, 10],\n",
    "#               'm__subsample': [0.7],\n",
    "#               'm__colsample_bytree': [0.7]\n",
    "#               }\n",
    "\n",
    "# # Using grid search CV, we can test for wide parameter ranges and estimate the best settings.\n",
    "# # VERY computationally expensive!\n",
    "\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# grid = GridSearchCV(pipeline, parameters, scoring='neg_root_mean_squared_error', verbose=3, refit=True)\n",
    "# grid.fit(X,y)\n",
    "\n",
    "# best_params = grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a model using the tuned hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBRegressor(colsample_bytree = 0.7, learning_rate = 0.05, max_depth = 5,\n",
    "                     min_child_weight = 10, n_estimators = 500, subsample = 0.7, \n",
    "                     random_state = 42, n_jobs = -1, verbosity = 2)\n",
    "\n",
    "pipeline = Pipeline(steps=[('s',scaler),('i', imputer), ('m', model)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test wells conditioning for the final leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the test well las files and save them as dataframes\n",
    "\n",
    "test = 'All Test Data'\n",
    "testfiles = glob.glob(test + \"/*.las\")\n",
    "\n",
    "i =1\n",
    "for testfile in testfiles:\n",
    "    las = lasio.read(testfile)\n",
    "    globals()[str(\"Test_Well_\"+str(i))] = las.df().reset_index().fillna(0)\n",
    "    globals()[str(\"Test_Well_\"+str(i))]['Well Name'] = str(testfile)[14:-4]\n",
    "    globals()[str(\"Test_Well_\"+str(i))]['Latitude'] = las.well.SLAT.value\n",
    "    globals()[str(\"Test_Well_\"+str(i))]['Longitude'] = las.well.SLON.value\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create exploratory dataframe for test wells to check on the logs available\n",
    "\n",
    "testwells = []\n",
    "lat = []\n",
    "lon = []\n",
    "keys = []\n",
    "logcount = []\n",
    "units = []\n",
    "uniqueunits = []\n",
    "descrs = []\n",
    "depth = []\n",
    "i = 1\n",
    "\n",
    "for testfile in testfiles:\n",
    "    testwell = str(testfile)[14:-4]\n",
    "    las = lasio.read(testfile)\n",
    "    latitude = las.well.SLAT.value\n",
    "    longitude = las.well.SLON.value\n",
    "    key = las.keys()\n",
    "    md = las.well.STOP.value\n",
    "    keycount = len(key)\n",
    "    unit = []\n",
    "    descr = []\n",
    "    for j in range(keycount):\n",
    "        unit.append(las.curves.items()[j][1].unit)\n",
    "        descr.append(las.curves.items()[j][1].descr)\n",
    "        j+=1\n",
    "    i+=1\n",
    "    uniqueunit = list(set(unit))\n",
    "    lat.append(latitude)\n",
    "    lon.append(longitude)\n",
    "    depth.append(md)\n",
    "    keys.append(key)\n",
    "    logcount.append(keycount)\n",
    "    testwells.append(testwell)\n",
    "    units.append(unit)\n",
    "    uniqueunits.append(uniqueunit)\n",
    "    descrs.append(descr)\n",
    "    \n",
    "test_welldf = DataFrame({'Wellname':testwells,'Latitude':lat,'Longitude':lon,'Depth':depth, 'Log Count':logcount,'Log List':keys,\n",
    "                    'Unit List':units, 'Unique Units':uniqueunits, 'Description': descrs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the lat,long and log count to get an idea on how the data is distributed\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.scatterplot(x ='Latitude', y = 'Longitude', data = welldf, size= 'Log Count', sizes = (100,600))\n",
    "sns.scatterplot(x ='Latitude', y = 'Longitude', data = test_welldf, size= 'Log Count', sizes = (100,600),color = 'red')\n",
    "\n",
    "# We can see that the relative locations of the test wells are well within the train dataset space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the log list column in our exploratory DF\n",
    "# We can try to figure out if the naming schemes of all the test well logs is similar.\n",
    "\n",
    "test_welldf_explode = test_welldf.explode('Log List')\n",
    "test_welldf_explode['Units'] = list(test_welldf.explode('Unit List')['Unit List'])\n",
    "test_welldf_explode['Description'] = list(test_welldf.explode('Description')['Description'])\n",
    "test_welldf_explode.rename(columns = {'Log List':'Logs'}, inplace = True)\n",
    "test_welldf_explode.drop(['Unit List'],axis = 1, inplace = True)\n",
    "\n",
    "# Append the las files for all the wells into a single dataframe\n",
    "\n",
    "test_well_logs = []\n",
    "for i in range (1, len(testwells)+1):\n",
    "    test_well_logs.append(globals()[\"Test_Well_\"+str(i)])\n",
    "    \n",
    "test_welldf_entire = pd.concat(test_well_logs)\n",
    "\n",
    "\n",
    "# Based on the units, lets select the log mnemonics that we can create a compound feature\n",
    "\n",
    "GAPI = list(set(test_welldf_explode[test_welldf_explode['Units'] == \"GAPI\"]['Logs'].value_counts().index))\n",
    "\n",
    "# Remove sprectral gamma ray logs from the list of logs having units GAPI\n",
    "SGR = ['SGRDD','HSGRD','HSGRS','SGRS','HSGR','SGR','HSGRR','MSGRR','SGRD','SGRDD','SGRR','HCGR']\n",
    "\n",
    "for element in GAPI:\n",
    "    if element in SGR:\n",
    "        GAPI.remove(element)\n",
    "\n",
    "DEC = list(set(test_welldf_explode[test_welldf_explode['Units'] == \"DEC\"]['Logs'].value_counts().index))\n",
    "\n",
    "IN = list(set(test_welldf_explode[test_welldf_explode['Units'] == \"IN\"]['Logs'].value_counts().index))\n",
    "\n",
    "OHMM = list(set(test_welldf_explode[test_welldf_explode['Units'] == \"OHMM\"]['Logs'].value_counts().index))\n",
    "\n",
    "GC3  = list(set(test_welldf_explode[test_welldf_explode['Units'] == \"G/C3\"]['Logs'].value_counts().index))\n",
    "\n",
    "# Remove density correction logs from the list of logs having units G/C3\n",
    "DCORR = ['HDRA','DRHO','DRH','ZCOR','DCOR','CORR','QRHO','QRHO_SLDT','QRHO']\n",
    "for element in GC3:\n",
    "    if element in DCORR:\n",
    "        GC3.remove(element)\n",
    "        \n",
    "BE = list(set(test_welldf_explode[test_welldf_explode['Units'] == \"B/E\"]['Logs'].value_counts().index))\n",
    "\n",
    "# Take the row average of the features to create average features if more than one kind of log is available\n",
    "\n",
    "test_welldf_entire['GR_average'] = test_welldf_entire[GAPI].mean(axis = 1)\n",
    "test_welldf_entire['PHI_average'] = test_welldf_entire[DEC].mean(axis = 1)\n",
    "test_welldf_entire['CALI_average'] = test_welldf_entire[IN].mean(axis = 1)\n",
    "test_welldf_entire['RES_average'] = test_welldf_entire[OHMM].mean(axis = 1)\n",
    "test_welldf_entire['RHOB_average'] = test_welldf_entire[GC3].mean(axis = 1)\n",
    "test_welldf_entire['PEF_average'] = test_welldf_entire[BE].mean(axis = 1)\n",
    "\n",
    "test_features =  test_welldf_entire[['DEPT','Latitude','Longitude','DTCO','RHOB_average','PEF_average',\n",
    "                                  'GR_average','PHI_average','CALI_average','RES_average']]\n",
    "\n",
    "# fill null values as zeroes\n",
    "test_features.fillna(0, axis =1, inplace = True)\n",
    "\n",
    "# Change negative values to zeroes\n",
    "test_features[test_features < 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the data too the pipeline using the complete dataset\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "# Predict the DTSM for test Wells\n",
    "\n",
    "test_predictions = pipeline.predict(test_features)\n",
    "\n",
    "# Make a Dataframe of predictions of test dataset\n",
    "submission_file = test_welldf_entire[['Well Name','DEPT']]\n",
    "submission_file['DTSM'] = test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature Importance and see if any insignificant features can be eliminated\n",
    "\n",
    "feat_imp = DataFrame(X.columns, columns=['Feature'])\n",
    "feat_imp['Feature Importance'] = pipeline.steps[2][1].feature_importances_\n",
    "\n",
    "feat_imp.sort_values(by = 'Feature Importance', ascending=False, inplace = True)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.barplot(x = 'Feature Importance', y = 'Feature', data = feat_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the cross-validation scores\n",
    "scores = cross_validate(pipeline, X, y, cv = 10, scoring = ('neg_root_mean_squared_error'))\n",
    "print(\"RMSE for xGBoost Regression: \",np.average(scores.get('test_score')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting the predicted well logs to their xlsx file\n",
    "for testwell in testwells:\n",
    "    submission_file[submission_file['Well Name'] == str(testwell)].drop('Well Name', axis = 1).to_excel(str(testwell)+'.xlsx', index = False)\n",
    "submission_file.to_csv('xgboost_tuned_new.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving all the conditioned data to save time and re-using a saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save conditioned and cleaned X and Y of bothe train and test datasets for easy reuse\n",
    "features_entire.to_csv('Train_Set_Conditioned.txt', index = False)\n",
    "test_features.to_csv('All_Test_Set_Conditioned.txt', index = False)\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Save to file in the current working directory\n",
    "pkl_filename = \"xgboost_model_tuned_new.pkl\"\n",
    "\n",
    "\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(pipeline, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from file\n",
    "pkl_filename = \"xgboost_model_tuned_new.pkl\"\n",
    "\n",
    "with open(pkl_filename, 'rb') as file:\n",
    "    pickle_model = pickle.load(file)\n",
    "    \n",
    "# Make submission file\n",
    "\n",
    "test_predictions = pickle_model.predict(test_features)\n",
    "submission_file = test_welldf_entire[['Well Name','DEPT']]\n",
    "submission_file['DTSM'] = test_predictions\n",
    "\n",
    "# Exporting the predicted well logs to their xlsx file\n",
    "for testwell in testwells:\n",
    "    submission_file[submission_file['Well Name'] == str(testwell)].drop('Well Name', axis = 1).to_excel(str(testwell)+'.xlsx', index = False)\n",
    "submission_file.to_csv('xgboost_tuned_new.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
