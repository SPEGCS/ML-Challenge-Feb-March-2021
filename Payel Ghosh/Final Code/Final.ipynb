{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the libraries used for the model.\n",
    "Using Python 3.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lasio  # ver 0.28\n",
    "import os     \n",
    "import numpy as np #ver 1.17.0\n",
    "import pandas as pd # ver 1.0.5\n",
    "import matplotlib.pyplot as plt #ver 3.2.2\n",
    "from math import isnan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all las files into an object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasAll = {}\n",
    "path='D:/New folder/ML Challenge Data' # path to the training data goes here\n",
    "for filename in os.listdir(path):\n",
    "    if filename.endswith(\".las\"):\n",
    "        lasAll[filename] = lasio.read(os.path.join(path, filename)) # creates an object containing data from all las files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of times each feature appears in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FeatureCount = {}   # initialize features count\n",
    "for filename in lasAll:\n",
    "    las = lasAll[filename]\n",
    "    for item in las.curves.iteritems():\n",
    "        if item[0] in FeatureCount.keys():\n",
    "            FeatureCount[item[0]] += 1  # Update FeatureCount if feature present in file\n",
    "        else:\n",
    "            FeatureCount[item[0]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a bar graph for the features that appear more than 10 times in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [] \n",
    "for key in FeatureCount:\n",
    "    if FeatureCount[key] > 10:\n",
    "        features.append(key)   # Add key to features, if FeatureCount>10\n",
    "values = [FeatureCount[key] for key in features]\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.bar(features, values)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "print(features) # array containing features appearing more than 10 times in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, plot the histogram of features from a sample las file to see the distribution functions. Note that some features are bimodal whereas some are unimodal. Also, some unimodal distributions are skewed. This, suggests that \"Median\" is a better measure of central tendency here compared to \"Mean\" which will be used later to replace missing values in the feature columns\n",
    "The pearson correlation coefficient is computed among all features. Note that 'DTCO','AFCO','NPHI' were found to be most correlated with 'DTSM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "las = lasAll['0f7a4609731a_TGS.las']  # randomly chosen las file \n",
    "logs = las.df()\n",
    "logs.hist(figsize=(10,10)) # plot the histogram of the features to see the distribution\n",
    "# Compute pearson correlation coefficient\n",
    "r = logs.corr(method=\"pearson\")\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create x and y data arrays with x containing frequently occuring predictor features as determined above and y the predicted parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlabels=['DEPT', 'DTCO', 'GRS', 'RXOZ', 'GRR', 'GRD', 'RHOZ', 'TNPH', 'DPHZ', 'HCAL', 'HDRA', 'PEFZ', \n",
    "         'SGRDD', 'TNPH_LS', 'RLA3', 'CALD', 'DPHZ_LS', 'CALR', 'SPR', 'AT30', 'AT90', 'AT10', 'AT20', 'HCALR',\n",
    "         'SPHI_LS', 'TENR', 'TENS', 'AT60', 'ILD', 'ILM', 'SFLU', 'CILD', 'NPHI_LS', 'HCALD', 'DTRP', 'DTRS', \n",
    "         'RHOB', 'TEND', 'PEF', 'DRHO', 'NPHI', 'DPHI', 'DPHI_LS', 'GR', 'DT', 'DTST', 'LLD', 'LLS', 'DTL', \n",
    "         'MSFL', 'RLA4', 'RLA5', 'GR_EDTC', 'HSGRD'] # 54 features that appear more than 10 times in the data\n",
    "\n",
    "ylabels = ['DTSM']\n",
    "x = []   # initialize input variable\n",
    "y = []   # initialize output variable\n",
    "\n",
    "for filename in lasAll:\n",
    "    lasAll[filename].df().fillna(lasAll[filename].df().median()) # replace missing values with median of columns    \n",
    "    allmeasures = list(lasAll[filename].df().columns)\n",
    "    if all([item in allmeasures for item in ylabels]):\n",
    "        for index, row in lasAll[filename].df().iterrows():\n",
    "            inputs = []\n",
    "            for i in xlabels:\n",
    "                try:\n",
    "                    inputs.append(row[allmeasures.index(i)]) # append rows of the data to the input array\n",
    "                except:\n",
    "                    inputs.append(np.nan)   # in case of an exception append a NaN\n",
    "            outputs = []\n",
    "            [outputs.append(row[allmeasures.index(i)]) for i in ylabels]\n",
    "            if all([not isnan(item) for item in outputs]):\n",
    "                x.append(inputs)   #append inputs to x if the value of DTSM is not NaN\n",
    "                y.append(outputs)  #append output to y if the value of DTSM is not NaN\n",
    "                \n",
    "x = np.asarray(x) # convert to an array\n",
    "y = np.asarray(y)\n",
    "print(np.shape(y)) # print shape of the arrays to check dimensions\n",
    "print(np.shape(x)) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Use a simple decision tree regressor to find feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor # scikit-learn ver 0.23.1\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=.15) # split arrays into random train and test sets\n",
    "\n",
    "train_x=np.nan_to_num(train_x) # convert any NaNs to numbers \n",
    "train_y=np.nan_to_num(train_y)\n",
    "test_x=np.nan_to_num(test_x)\n",
    "test_y=np.nan_to_num(test_y)\n",
    "\n",
    "scaler = MinMaxScaler() #data_scaled = X_std * (max - min) + min, where min,max correspond to the data range\n",
    "# transform data\n",
    "train_x = scaler.fit_transform(train_x) # scale the data using the data range\n",
    "test_x = scaler.fit_transform(test_x)\n",
    "\n",
    "\n",
    "\n",
    "regressor = DecisionTreeRegressor(random_state=42) # initialise the DT regressor. Random state controls the randomness of the estimator\n",
    "regressor.fit(train_x,np.ravel(train_y)) # train the model using the training data\n",
    "\n",
    "pred = regressor.predict(test_x) # make predictions on the test data\n",
    "rmse = np.sqrt(MSE(np.ravel(test_y),pred)) # evaluate the performance on the test data\n",
    "print(rmse)\n",
    "\n",
    "importance = regressor.feature_importances_  # compute feature importance. Feaures at the root of the tree are more important than ones used near the leaves.\n",
    "imp=np.array(importance)\n",
    "sort_index=np.argsort(imp) # sort the importance form low to high\n",
    "print(sort_index) # show the indices of important features\n",
    "\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "\tprint('Feature: %0d, Score: %.5f' % (i,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The relative distance between the wells is preserved in the latitude and longitude values from the file headers, \n",
    "#which is reflected in the scatter plot showing the cluster of well positions.\n",
    "\n",
    "Latitude = []\n",
    "Longitude = []\n",
    "for filename in lasAll:\n",
    "    las = lasAll[filename]\n",
    "    Latitude.append(las.well['SLAT'].value) # for each well get the latitude and longitude information\n",
    "    Longitude.append(las.well['SLON'].value)\n",
    "\n",
    "plt.scatter(x=Longitude,y=Latitude)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top ten most important features found from the decision tree regressor were: 'DTCO','GRS','HCALR','TNPH','TNPH_LS','SPR','HSGRD','AT10','HCALD','DT'\n",
    "\n",
    "The top three correlated features to DTSM were: DTCO, AFCO and NPHI\n",
    "\n",
    "Taking a union of these features gives a total of 12 features. Create another variable x with only these selected features+ Latitude and longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 important features from decision tree (union) 3 featres that are most correlated =12 featuress \n",
    "ylabels = ['DTSM']\n",
    "xlabels=['DTCO','GRS','HCALR','TNPH','TNPH_LS','SPR','HSGRD','AT10','HCALD','DT','AFCO','NPHI'] \n",
    "x = []\n",
    "y = []\n",
    "Lat=[]\n",
    "Lon=[]\n",
    "for filename in lasAll:\n",
    "    lasAll[filename].df().fillna(lasAll[filename].df().median()) # replace missing values with median of columns\n",
    "    allmeasures = list(lasAll[filename].df().columns)\n",
    "    if all([item in allmeasures for item in ylabels]):\n",
    "        for index, row in lasAll[filename].df().iterrows():\n",
    "            inputs = []\n",
    "            for i in xlabels:\n",
    "                try:\n",
    "                    inputs.append(row[allmeasures.index(i)])\n",
    "                except:\n",
    "                    inputs.append(np.nan)\n",
    "                    \n",
    "            outputs = []\n",
    "            [outputs.append(row[allmeasures.index(i)]) for i in ylabels]\n",
    "            if all([not isnan(item) for item in outputs]):\n",
    "                x.append(inputs) # append 12 features from all files\n",
    "                y.append(outputs) # append DTSM from all files\n",
    "                Lat.append(lasAll[filename].well['SLAT'].value) # obtain position informtion from all files\n",
    "                Lon.append(lasAll[filename].well['SLON'].value)\n",
    "x = np.asarray(x)\n",
    "y = np.asarray(y)\n",
    "x=np.c_[ x, np.ravel(Lat),np.ravel(Lon)] # concatenate latitude and Longitude to data array\n",
    "print(np.shape(y))\n",
    "print(np.shape(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training and test data from the subset of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing  # scikit-learn ver 0.23.1\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(x,y,test_size=0.3,random_state=42) # split arrays into random train and test sets\n",
    "train_x=np.nan_to_num(train_x) # convert NaNs to numbers\n",
    "test_x=np.nan_to_num(test_x)\n",
    "scaler = preprocessing.StandardScaler()\n",
    "# transform data\n",
    "train_x = scaler.fit_transform(train_x) #scale the data. Remove the mean and make the data have unit variance\n",
    "test_x = scaler.fit_transform(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model#1 XGBoost regressor using the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance # ver 1.2.0\n",
    "import xgboost as xg\n",
    "from matplotlib import pyplot # ver 3.2.2\n",
    "xgb_regressor = xg.XGBRegressor(objective='reg:squarederror', n_estimators=100,learning_rate=0.5,importance_type='gain',feature_selector='shuffle')\n",
    "    # initialize the ensemble regressor with 100 estimators\n",
    "\n",
    "xgb_regressor.fit(train_x,train_y) # train the regressor using the training data\n",
    "\n",
    "pred = xgb_regressor.predict(test_x)\n",
    "rmse1 = np.sqrt(MSE(test_y,pred))\n",
    "print(rmse1)\n",
    "\n",
    "# Plot the original versus precdictions\n",
    "x_ax = range(len(test_y))\n",
    "plt.plot(x_ax, test_y, label=\"original\")\n",
    "plt.plot(x_ax, pred, label=\"predicted\")\n",
    "plt.title(\"DTSM test and predicted data\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "score = xgb_regressor.score(train_x, train_y)\n",
    "print(\"R-squared:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Model#2 Random Forest Regressor using the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor # scikit-learn ver 0.23.1\n",
    "\n",
    "rf_regressor = RandomForestRegressor(bootstrap=True, criterion='mse',\n",
    "                      n_estimators=100) #initialize rf regressor\n",
    "\n",
    "rf_regressor.fit(train_x, np.ravel(train_y)) # train rf regressor using training data\n",
    "ypred = rf_regressor.predict(test_x) # make predictions on the test data\n",
    "\n",
    "score = rf_regressor.score(train_x, train_y) # compute training performance\n",
    "print(\"R-squared:\", score)\n",
    "\n",
    "rmse2 = np.sqrt(MSE(np.ravel(test_y), ypred)) # compute test performance\n",
    "print(\"RMSE= \",rmse2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the stacked model using Model#1, Model#2, and Lasso regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingRegressor # scikit-learn ver 0.23.1\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "lasso = LassoCV(n_alphas=10, eps=1e-3, max_iter=100, precompute=True) # initialize lasso regressor\n",
    "\n",
    "estimators = [('Random Forest', rf_regressor),\n",
    "              ('Lasso',lasso),\n",
    "              ('Gradient Boosting', xgb_regressor)] # create a stack of regressors\n",
    "\n",
    "stacking_regressor = StackingRegressor(estimators=estimators, \n",
    "                                       final_estimator=RidgeCV())\n",
    "stacking_regressor.fit(train_x,np.ravel(train_y)) # train the stacking regressor\n",
    "\n",
    "ypred3 = stacking_regressor.predict(test_x) # make predictions on the test data using the stacking regressor\n",
    "rmse3 = np.sqrt(MSE(np.ravel(test_y), ypred3)) # compute test performance\n",
    "print(\"RMSE= \",rmse3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions and save excel files from the Leaderboard files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "path='D:/New folder/Leaderboard' # Path containing Leaderboard files\n",
    "lasDict = {}   # Initialize a new object for loading Leaderboard files\n",
    "scaler = preprocessing.StandardScaler()  # use standard scaler to remove mean and make data have unit variance\n",
    "\n",
    "for filename in os.listdir(path):\n",
    "    if filename.endswith(\".las\"):\n",
    "        lasDict[filename] = lasio.read(os.path.join(path, filename))\n",
    "\n",
    "\n",
    "xlabels=['DTCO','GRS','HCALR','TNPH','TNPH_LS','SPR','HSGRD','AT10','HCALD','DT','AFCO','NPHI']    \n",
    "\n",
    "for filename in lasDict:\n",
    "    print(filename)\n",
    "    df2=lasDict[filename].df()\n",
    "    print(df2.shape)\n",
    "    x1=[]   # initialize data arrays\n",
    "    Lat=[] \n",
    "    Lon=[]\n",
    "    df2.fillna(df2.median()) #replace missing values with median of columns\n",
    "    allmeasures = list(lasDict[filename].df().columns)\n",
    "    for index, row in lasDict[filename].df().iterrows():\n",
    "        inputs = []   \n",
    "        for i in xlabels:          \n",
    "            try:\n",
    "                inputs.append(row[allmeasures.index(i)])\n",
    "            except:\n",
    "                inputs.append(np.nan)\n",
    "        x1.append(inputs)    # append input to array x\n",
    "        Lat.append(lasDict[filename].well['SLAT'].value) # append latitude information to Lat\n",
    "        Lon.append(lasDict[filename].well['SLON'].value) # append longitude information to Lon\n",
    "    \n",
    "    x1 = np.asarray(x1) # convert to array\n",
    "    x1=np.c_[ x1, np.ravel(Lat), np.ravel(Lon) ] # concatenate Latitude and Longitude to data array\n",
    "    print(np.shape(x1))\n",
    "    \n",
    "    x1=np.nan_to_num(x1) # convert NaNs to nums\n",
    "    x1 = scaler.fit_transform(x1) # scale data to make mean=0 and var=1   \n",
    "    print(np.shape(x1))\n",
    "\n",
    "    pred=stacking_regressor.predict(x1) # apply the stacked model on the Leaderboard data\n",
    "    print(np.shape(pred))\n",
    "        \n",
    "    df = pd.DataFrame(pred,index=df2.index,columns=['DTSM']) # create a new dataframe to save output files\n",
    "    name=x = filename.split(\".\") # get the filename from the las files\n",
    "    name=name[0]+'.xlsx'\n",
    "    print(name)\n",
    "    df.to_excel(os.path.join(path, name), index = True,header=True) # save outputs to excel files. Header=True makes sure the depths are saved out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
